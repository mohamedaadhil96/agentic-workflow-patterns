{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9f346b2",
   "metadata": {},
   "source": [
    "## Agentic Workflow Design Pattern - Prompt Chaining\n",
    "\n",
    "Prompt Chaining is a foundational design pattern in agentic AI workflows. It breaks down complex tasks into a sequence of smaller, interconnected prompts, where the output of one large language model (LLM) call becomes the input for the next.\n",
    "\n",
    "This approach enables structured, step-by-step reasoning and improves reliability, controllability, and performance compared to handling everything in a single large prompt.\n",
    "\n",
    "### How It Works\n",
    "In prompt chaining:\n",
    "\n",
    "1. A complex goal decomposes into logical subtasks.\n",
    "2. Each subtask is handled by a specialized prompt.\n",
    "3. Outputs are passed sequentially (often formatted as structured data like JSON for easy parsing).\n",
    "4. The chain continues until the final result is produced.\n",
    "\n",
    "This mimics a pipeline or assembly line, allowing focused processing at each stage. It evolved from techniques like Chain-of-Thought prompting but emphasizes modular, sequential execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7848a646",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e77511ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedState(TypedDict):\n",
    "    query: str\n",
    "    model: ChatGroq\n",
    "    from_ml_topic: bool\n",
    "    ai_answer: str\n",
    "\n",
    "class GraderOutput(TypedDict):\n",
    "    from_machine_learning_topic: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69f0751e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(shared_state: SharedState):\n",
    "    model = ChatGroq(model_name=\"llama-3.3-70b-versatile\")\n",
    "    shared_state['model'] = model\n",
    "    return shared_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89607207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_topic(shared_state: SharedState):\n",
    "    print(\"Determining if the query is related to machine learning topics...\")\n",
    "    \n",
    "    prompt = \"\"\"\n",
    "        You are a classifier that determines if a user's query is related to machine learning topics.\n",
    "        Given the user's query, return True if it is related to machine learning topics, otherwise return False.\n",
    "    \"\"\"\n",
    "\n",
    "    model = shared_state['model']\n",
    "    structured_llm_grader = model.with_structured_output(GraderOutput)\n",
    "    query = shared_state['query']\n",
    "\n",
    "    grade_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", prompt),\n",
    "            (\"human\", \"User's Query: \\n\\n {query}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    retrieval_grader = grade_prompt | structured_llm_grader\n",
    "\n",
    "    result = retrieval_grader.invoke({\"query\": query})\n",
    "    shared_state['from_ml_topic'] = result['from_machine_learning_topic']\n",
    "\n",
    "    return shared_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12077d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grader_node(shared_state: SharedState):\n",
    "    if shared_state['from_ml_topic']:\n",
    "        return \"continue\"\n",
    "\n",
    "    return \"exit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1704a6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(shared_state: SharedState):\n",
    "    print(\"Answering the user's query...\")\n",
    "    prompt = \"\"\"\n",
    "        You are an expert in machine learning. Answer the user's query under 200 words.\n",
    "    \"\"\"\n",
    "    model = shared_state['model']\n",
    "    query = shared_state['query']\n",
    "    answer_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", prompt),\n",
    "            (\"human\", \"User's Query: \\n\\n {query}\"),\n",
    "        ]\n",
    "    )\n",
    "    answer_chain = answer_prompt | model | StrOutputParser()\n",
    "    result = answer_chain.invoke({\"query\": query})\n",
    "    shared_state['ai_answer'] = result\n",
    "\n",
    "    return shared_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4437a9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph():\n",
    "    workflow = StateGraph(SharedState)\n",
    "\n",
    "    # Add Nodes\n",
    "    workflow.add_node(build_model, \"build_model\")\n",
    "    workflow.add_node(get_query_topic, \"get_query_topic\")\n",
    "    workflow.add_node(answer_query, \"answer_query\")\n",
    "\n",
    "    workflow.add_edge(START, \"build_model\")\n",
    "    workflow.add_edge(\"build_model\", \"get_query_topic\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"get_query_topic\", \n",
    "        grader_node, \n",
    "        { \n",
    "            \"continue\": \"answer_query\",\n",
    "            \"exit\": END \n",
    "        }\n",
    "    )\n",
    "    workflow.add_edge(\"answer_query\", END)\n",
    "\n",
    "    return workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b8558e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 1: \"What are the latest advancements in machine learning?\"\n",
    "# Query 2: \"What is the capital of Sri lanka?\"\n",
    "def execute_prompt_chain_workflow():\n",
    "    workflow = build_graph()\n",
    "    initial_state: SharedState = {\n",
    "        \"query\": \"What is the capital of Sri lanka?\",\n",
    "    }\n",
    "\n",
    "    agent_response = workflow.invoke(initial_state)\n",
    "    print(agent_response)\n",
    "\n",
    "    if agent_response['from_ml_topic']:\n",
    "        print(\"AI's Answer:\", agent_response['ai_answer'])\n",
    "    else:\n",
    "        print(\"The query is not related to machine learning topics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc4c34ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determining if the query is related to machine learning topics...\n",
      "{'query': 'What is the capital of Sri lanka?', 'model': ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 32768, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x00000204968D8A10>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000204968D8EC0>, model_name='llama-3.3-70b-versatile', model_kwargs={}, groq_api_key=SecretStr('**********')), 'from_ml_topic': False}\n",
      "The query is not related to machine learning topics.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "execute_prompt_chain_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f825349b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
